{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg_words_write(contents):\n",
    "    contents_segs = []\n",
    "    stopwords = getStopwords()\n",
    "    f = open('contents.txt','wb')\n",
    "    for content in contents:\n",
    "        c = re.sub(' ', '', content)\n",
    "        segs = list(jieba.lcut(c)) #Sentence segment\n",
    "        sentence_segment = []\n",
    "        for word in segs:\n",
    "            if word not in stopwords:\n",
    "                sentence_segment.append(word) \n",
    "        contents_segs.append(\" \".join(sentence_segment))\n",
    "        f.write(\" \".join(sentence_segment).encode('utf-8')) #Write sentence segment as contents.txt\n",
    "    f.close\n",
    "    return contents_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg_words(contents):\n",
    "    contents_segs = []\n",
    "    stopwords = getStopwords()\n",
    "    for content in contents:\n",
    "        c = re.sub(' ', '', content)\n",
    "        segs = list(jieba.lcut(c))\n",
    "        sentence_segment = []\n",
    "        for word in segs:\n",
    "            if word not in stopwords:\n",
    "                sentence_segment.append(word)\n",
    "        contents_segs.append(\" \".join(sentence_segment)) #Get sentence segment without writing\n",
    "    return contents_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopwords(): #Get stop words list\n",
    "    stopwords = []\n",
    "    with open(\"chineseStopWords.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            stopwords.append(line.strip())\n",
    "    stopwords.append('\"')\n",
    "    stopwords.append('\\n')\n",
    "    stopwords.append('～')\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_vectorize(size,contents,model):\n",
    "    contents_vector = np.zeros((len(contents),size))\n",
    "    for i in range(len(contents)):\n",
    "        n = 0\n",
    "        segs = contents[i].split()\n",
    "        vector_sum = np.zeros(size)\n",
    "        for j in range(len(segs)):\n",
    "            try:\n",
    "                vector_sum = vector_sum + model[segs[j]] #Sum word vector\n",
    "                n = n + 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        contents_vector[i] = vector_sum/n \n",
    "    return contents_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word_to_Vec():\n",
    "    def __init__(self,embedder = Word2Vec(),size = 0,contents_segs = []):\n",
    "        self.embedder = embedder\n",
    "        self.size = size\n",
    "    \n",
    "    def fit(self, X, seg_contents_exist = False, size = 256, min_count=5, iter=10): \n",
    "        #size is ncol of word vector, min_count represent the minimum appearence of a word be count in the model\n",
    "        self.size = 256\n",
    "        if(seg_contents_exist == False):\n",
    "            if os.path.exists(\"content.txt\"):\n",
    "                os.remove(\"content.txt\")\n",
    "            else:\n",
    "                pass\n",
    "            contents_segs = seg_words_write(X)\n",
    "        else:\n",
    "            contents_segs = seg_words(X)\n",
    "        self.embedder = Word2Vec(PathLineSentences(\"contents.txt\"),size=size, window=10, min_count=min_count, iter=iter)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        contents_segs = seg_words(X)\n",
    "        return sentence_vectorize(self.size,contents_segs,self.embedder)\n",
    "    \n",
    "    def fit_transform(self, X, seg_contents_exist = False, size = 256, min_count=5, iter=10):\n",
    "        self.size = 256\n",
    "        if(seg_contents_exist == False):\n",
    "            if os.path.exists(\"content.txt\"):\n",
    "                os.remove(\"content.txt\")\n",
    "            else:\n",
    "                pass\n",
    "            contents_segs = seg_words_write(X)\n",
    "        else:\n",
    "            contents_segs = seg_words(X)\n",
    "        # I still need to do word2vec by reading data from a file directly. Don't know how to handle it\n",
    "        self.embedder = Word2Vec(PathLineSentences(\"contents.txt\"),size=size, window=10, min_count=min_count, iter=iter) \n",
    "        return sentence_vectorize(self.size,contents_segs,self.embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sentiment_analysis_trainingset.csv\", encoding=\"utf-8\") \n",
    "text = data['content'] #Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valdf = pd.read_csv(\"sentiment_analysis_validationset.csv\", encoding=\"utf-8\") \n",
    "text_val = valdf['content'] #Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = Word_to_Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZHANKE~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.876 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "X = transformer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = transformer.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('午饭', 0.8135820627212524),\n",
       " ('晚饭', 0.7785147428512573),\n",
       " ('吃晚饭', 0.6976807117462158),\n",
       " ('就近', 0.6894402503967285),\n",
       " ('办事', 0.6685695052146912),\n",
       " ('肚子饿', 0.658237874507904),\n",
       " ('早饭', 0.6402450799942017),\n",
       " ('觅食', 0.6394287347793579),\n",
       " ('顺便来', 0.6358612179756165),\n",
       " ('顺道', 0.629756510257721)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.embedder.most_similar('中饭') #Get most similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec + RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_traffic_convenience\n",
      "0.874933333333\n",
      "[[11584     0     0   173]\n",
      " [  133     0     0    49]\n",
      " [  100     0     0    36]\n",
      " [ 1385     0     0  1540]]\n",
      "location_distance_from_business_district\n",
      "0.8296\n",
      "[[11781     0     0   251]\n",
      " [   88     0     0     2]\n",
      " [   70     0     0    10]\n",
      " [ 2135     0     0   663]]\n",
      "location_easy_to_find\n",
      "0.821933333333\n",
      "[[11257     6     0   253]\n",
      " [  414    23     0   115]\n",
      " [  251     4     0    74]\n",
      " [ 1553     1     0  1049]]\n",
      "service_wait_time\n",
      "0.887866666667\n",
      "[[13216     7    10     4]\n",
      " [  380    60    20     5]\n",
      " [  549    11    31     8]\n",
      " [  674     8     6    11]]\n",
      "service_waiters_attitude\n",
      "0.695666666667\n",
      "[[5455   26    4  507]\n",
      " [ 361  529   14  305]\n",
      " [ 868  136   17  809]\n",
      " [1494   38    3 4434]]\n",
      "service_parking_convenience\n",
      "0.940066666667\n",
      "[[14042     0     0     4]\n",
      " [  168     0     0    20]\n",
      " [  186     0     0    18]\n",
      " [  503     0     0    59]]\n",
      "service_serving_speed\n",
      "0.8674\n",
      "[[12633    13     0    31]\n",
      " [  629    81     0    95]\n",
      " [  311     5     0    52]\n",
      " [  847     6     0   297]]\n",
      "price_level\n",
      "0.6048\n",
      "[[7154   36  221   86]\n",
      " [ 799  342  525   94]\n",
      " [2063  193 1036  223]\n",
      " [1171   61  456  540]]\n",
      "price_cost_effective\n",
      "0.795933333333\n",
      "[[11182     0     0   246]\n",
      " [  377     1     0    67]\n",
      " [  315     0     0    83]\n",
      " [ 1973     0     0   756]]\n",
      "price_discount\n",
      "0.7344\n",
      "[[8960    0  135  167]\n",
      " [ 156    0   73   37]\n",
      " [1635    0  707  289]\n",
      " [1185    0  307 1349]]\n",
      "environment_decoration\n",
      "0.731\n",
      "[[7085    0    1  715]\n",
      " [ 166    0    1  108]\n",
      " [ 643    0    2  667]\n",
      " [1734    0    0 3878]]\n",
      "environment_noise\n",
      "0.7448\n",
      "[[9960    0    0  561]\n",
      " [ 371    0    0  110]\n",
      " [ 505    0    0  160]\n",
      " [2121    0    0 1212]]\n",
      "environment_space\n",
      "0.6744\n",
      "[[8843    7    9  632]\n",
      " [ 593   20    7  154]\n",
      " [1009    3   15  283]\n",
      " [2177    3    7 1238]]\n",
      "environment_cleaness\n",
      "0.734933333333\n",
      "[[8987    3    0  606]\n",
      " [ 403   21    0  201]\n",
      " [ 422    1    0  204]\n",
      " [2136    0    0 2016]]\n",
      "dish_portion\n",
      "0.648266666667\n",
      "[[7591   27    1  493]\n",
      " [ 938  160    0  344]\n",
      " [ 809   33    0  563]\n",
      " [2047   21    0 1973]]\n",
      "dish_taste\n",
      "0.6714\n",
      "[[  50   22  193  491]\n",
      " [   2   42  455   81]\n",
      " [  10   11 3479 2320]\n",
      " [   6    7 1331 6500]]\n",
      "dish_look\n",
      "0.7482\n",
      "[[10425     0     0   333]\n",
      " [  409     0     0    46]\n",
      " [  582     0     0    79]\n",
      " [ 2328     0     0   798]]\n",
      "dish_recommendation\n",
      "0.817333333333\n",
      "[[11928     0     0   155]\n",
      " [  298     0     0    37]\n",
      " [  211     0     0    76]\n",
      " [ 1963     0     0   332]]\n",
      "others_overall_experience\n",
      "0.7524\n",
      "[[   0    8   28  249]\n",
      " [   0  687  317  279]\n",
      " [   0  171  852 2334]\n",
      " [   0   30  298 9747]]\n",
      "others_willing_to_consume_again\n",
      "0.715\n",
      "[[8604    4    0  746]\n",
      " [ 525    9    0   43]\n",
      " [ 341    2    0   52]\n",
      " [2562    0    0 2112]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    j = i + 2\n",
    "    y = list(data[data.columns[j]])\n",
    "    y_val = list(valdf[data.columns[j]])\n",
    "    clf = RidgeClassifier().fit(X, y)\n",
    "    clf.fit(X, y)\n",
    "    print(data.columns[j]) #col name\n",
    "    print(clf.score(X_val,y_val)) # score\n",
    "    print(confusion_matrix(y_val,clf.predict(X_val))) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_traffic_convenience\n",
      "0.882066666667\n",
      "[[11445     0     0   312]\n",
      " [  124     0     0    58]\n",
      " [   92     0     0    44]\n",
      " [ 1139     0     0  1786]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_distance_from_business_district\n",
      "0.831533333333\n",
      "[[11714     0     0   318]\n",
      " [   87     0     0     3]\n",
      " [   68     0     0    12]\n",
      " [ 2039     0     0   759]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_easy_to_find\n",
      "0.829733333333\n",
      "[[11192    21     0   303]\n",
      " [  382    58     0   112]\n",
      " [  239    14     0    76]\n",
      " [ 1402     5     0  1196]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_wait_time\n",
      "0.8902\n",
      "[[13178    22    15    22]\n",
      " [  343   106    12     4]\n",
      " [  512    42    28    17]\n",
      " [  634    17     7    41]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_waiters_attitude\n",
      "0.721333333333\n",
      "[[5356   58   10  568]\n",
      " [ 211  671   18  309]\n",
      " [ 634  240   44  912]\n",
      " [1112   90   18 4749]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_parking_convenience\n",
      "0.947866666667\n",
      "[[14028     0     0    18]\n",
      " [  111     1     0    76]\n",
      " [  151     0     0    53]\n",
      " [  373     0     0   189]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_serving_speed\n",
      "0.8756\n",
      "[[12573    33     0    71]\n",
      " [  556   162     0    87]\n",
      " [  288    20     0    60]\n",
      " [  727    24     0   399]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_level\n",
      "0.625\n",
      "[[6999   62  297  139]\n",
      " [ 614  475  550  121]\n",
      " [1729  261 1229  296]\n",
      " [ 952   95  509  672]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_cost_effective\n",
      "0.7982\n",
      "[[11090     1     0   337]\n",
      " [  369     1     1    74]\n",
      " [  297     2     0    99]\n",
      " [ 1847     0     0   882]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_discount\n",
      "0.747533333333\n",
      "[[8817    0  191  254]\n",
      " [ 138    0   86   42]\n",
      " [1372    0  860  399]\n",
      " [ 956    0  349 1536]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment_decoration\n",
      "0.739133333333\n",
      "[[6975    0    4  822]\n",
      " [ 150    0    2  123]\n",
      " [ 578    0    5  729]\n",
      " [1505    0    0 4107]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment_noise\n",
      "0.749066666667\n",
      "[[9886    1    0  634]\n",
      " [ 354    7    0  120]\n",
      " [ 488    1    0  176]\n",
      " [1990    0    0 1343]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment_space\n",
      "0.675333333333\n",
      "[[8783   20   14  674]\n",
      " [ 579   38    9  148]\n",
      " [ 990   12   17  291]\n",
      " [2113   12    8 1292]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment_cleaness\n",
      "0.7408\n",
      "[[8885   14    0  697]\n",
      " [ 369   66    0  190]\n",
      " [ 397    4    0  226]\n",
      " [1985    6    0 2161]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish_portion\n",
      "0.654133333333\n",
      "[[7474   57    1  580]\n",
      " [ 854  247    2  339]\n",
      " [ 739   62    2  602]\n",
      " [1904   47    1 2089]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish_taste\n",
      "0.680533333333\n",
      "[[ 138   30  150  438]\n",
      " [  17  115  381   67]\n",
      " [  39   27 3510 2244]\n",
      " [  33    9 1357 6445]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish_look\n",
      "0.750066666667\n",
      "[[10387     1     0   370]\n",
      " [  405     3     0    47]\n",
      " [  572     0     0    89]\n",
      " [ 2265     0     0   861]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish_recommendation\n",
      "0.8198\n",
      "[[11872     0     0   211]\n",
      " [  290     0     0    45]\n",
      " [  201     0     0    86]\n",
      " [ 1870     0     0   425]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "others_overall_experience\n",
      "0.7608\n",
      "[[   0   12   33  240]\n",
      " [   0  811  262  210]\n",
      " [   0  256  953 2148]\n",
      " [   0   50  377 9648]]\n",
      "others_willing_to_consume_again\n",
      "0.718733333333\n",
      "[[8533   18    0  803]\n",
      " [ 502   38    0   37]\n",
      " [ 335    6    0   54]\n",
      " [2461    3    0 2210]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    j = i + 2\n",
    "    y = list(data[data.columns[j]])\n",
    "    y_val = list(valdf[data.columns[j]])\n",
    "    clf =  LinearSVC(random_state=0,max_iter=700)\n",
    "    clf.fit(X, y)\n",
    "    print(data.columns[j])\n",
    "    print(clf.score(X_val,y_val))\n",
    "    print(confusion_matrix(y_val,clf.predict(X_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
