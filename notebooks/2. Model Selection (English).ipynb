{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('jieba').setLevel(logging.WARN)\n",
    "logging.getLogger('fgclassifier').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use our baseline model.\n",
    "It also demonstrates how to test different feature models (i.e.,\n",
    "different ways of building the features) at the same time.\n",
    "\n",
    "We will use mostly the Google Translated English dataset for this\n",
    "demonstration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 20:49:37,309 [INFO] Reading /opt/storage/english_train.csv..\n",
      "2018-12-05 20:49:37,616 [INFO] Reading /opt/storage/english_valid.csv..\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fgclassifier.utils import read_data, get_dataset\n",
    "\n",
    "X_train, y_train = read_data(get_dataset('train_en'))\n",
    "X_test, y_test = read_data(get_dataset('valid_en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del fm['tfidf_sv']\n",
    "# del fm['tfidf_sv_dense']\n",
    "# del fm['lsa_200_sv']\n",
    "# del fm['lsa_500_sv']\n",
    "# del fm['count_tiny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache feature models and trained fetures, we make this cache object\n",
    "# so different steps can reuse previously transformed features\n",
    "fm = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 20:49:38,572 [INFO] 'pattern' package not found; tag filters are not available for English\n",
      "2018-12-05 20:49:38,585 [INFO] Building features for count_en...\n",
      "2018-12-05 20:50:05,052 [INFO] Vocab Size: 4000\n",
      "2018-12-05 20:50:06,219 [INFO] Building features for count_en_sv...\n",
      "2018-12-05 20:50:29,361 [INFO] Vocab Size: 2000\n"
     ]
    }
   ],
   "source": [
    "from fgclassifier.features import FeaturePipeline, logger\n",
    "\n",
    "def build_features(fm_names, fm):\n",
    "    for name in fm_names:\n",
    "        logger.info(f'Building features for {name}...')\n",
    "        model = FeaturePipeline.from_spec(name, cache=fm)\n",
    "        model.fit_transform(X_train)\n",
    "        model.transform(X_test)\n",
    "    \n",
    "build_features(['count_en', 'count_en_sv'], fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exam the quality of the top terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (8000,) (2000,)\n",
      "\n",
      "min_df: 3.000, max_df: 1.000, ngram_range: (1, 6)\n",
      "\n",
      "vocab size: 4000\n",
      "\n",
      "\n",
      "Stop words size: 3712557\n",
      "\n",
      "good \t 18975\n",
      "taste \t 13095\n",
      "eat \t 11877\n",
      "time \t 7277\n",
      "delicious \t 7088\n",
      "\n",
      "min_df: 3.000, max_df: 1.000, ngram_range: (1, 6)\n",
      "\n",
      "vocab size: 2000\n",
      "\n",
      "\n",
      "Stop words size: 3714557\n",
      "\n",
      "good \t 18975\n",
      "taste \t 13095\n",
      "eat \t 11877\n",
      "time \t 7277\n",
      "delicious \t 7088\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Data Shape:', X_train.shape, X_test.shape)\n",
    "\n",
    "for mn in ['count_en', 'count_en_sv']:\n",
    "    model = fm[mn]['model'].named_steps[mn]\n",
    "    key = next(filter(lambda x: 'fit_transform' in x, fm[mn].keys()))\n",
    "    x_train = fm[mn][key]\n",
    "    counts = np.sum(x_train, axis=0).flat\n",
    "    counts = {k: counts[v] for k, v in model.vocabulary_.items()}\n",
    "    print('\\nmin_df: %.3f, max_df: %.3f, ngram_range: %s' % (\n",
    "        model.min_df, model.max_df, model.ngram_range\n",
    "    ))\n",
    "    \n",
    "    print('\\nvocab size: %s\\n' % len(model.vocabulary_))\n",
    "    if model.stop_words_:\n",
    "        print('\\nStop words size: %s\\n' % len(model.stop_words_))\n",
    "    \n",
    "    # Remove stop words as they take a lot of memory\n",
    "    model.stop_words_ = None\n",
    "    \n",
    "    print('\\n'.join([\n",
    "        '%s \\t %s' % (k, v)\n",
    "        for k, v in Counter(counts).most_common()[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the word count features, as it is pretty slow to run for the whole dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/fm_cache-count_en\n",
      "Saved data/fm_cache-count_en_sv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2000x4000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 280136 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "def partial_get(d, keyword):\n",
    "    key = next(filter(lambda x: x.startswith(keyword), d.keys()))\n",
    "    return d[key]\n",
    "\n",
    "def save_transform_cache(mn, path=None):\n",
    "    path = path or f'data/fm_cache-{mn}'\n",
    "    Xtrain = partial_get(fm[mn], 'fit_transform')\n",
    "    Xtest = partial_get(fm[mn], 'transform')\n",
    "    joblib.dump(Xtrain, path + '-train.pkl')\n",
    "    joblib.dump(Xtest, path + '-test.pkl')\n",
    "    print(f'Saved {path}')\n",
    "    \n",
    "save_transform_cache('count_en')\n",
    "save_transform_cache('count_en_sv')\n",
    "joblib.load('data/fm_cache-count-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:11:08,921 [INFO] Building features for tfidf_en...\n",
      "2018-12-05 21:11:08,922 [INFO]   tfidf_en: fit_transform use cache.\n",
      "2018-12-05 21:11:08,923 [INFO]   tfidf_en: transform use cache.\n",
      "2018-12-05 21:11:08,923 [INFO] Building features for tfidf_en_dense...\n",
      "2018-12-05 21:11:08,924 [INFO]   tfidf_en_dense: fit_transform use cache.\n",
      "2018-12-05 21:11:08,925 [INFO]   tfidf_en_dense: transform use cache.\n",
      "2018-12-05 21:11:08,925 [INFO] Building features for lsa_500_en...\n",
      "2018-12-05 21:11:08,926 [INFO]   lsa_500_en: fit_transform use cache.\n",
      "2018-12-05 21:11:08,927 [INFO]   lsa_500_en: transform use cache.\n",
      "2018-12-05 21:11:08,928 [INFO] Building features for lsa_1k_en...\n",
      "2018-12-05 21:11:08,928 [INFO]   lsa_1k_en: fit_transform use cache.\n",
      "2018-12-05 21:11:08,929 [INFO]   lsa_1k_en: transform use cache.\n",
      "2018-12-05 21:11:08,931 [INFO] Building features for tfidf_en_sv...\n",
      "2018-12-05 21:11:08,935 [INFO]   tfidf_en_sv: fit_transform use cache.\n",
      "2018-12-05 21:11:08,942 [INFO]   tfidf_en_sv: transform use cache.\n",
      "2018-12-05 21:11:08,944 [INFO] Building features for tfidf_en_sv_dense...\n",
      "2018-12-05 21:11:08,947 [INFO]   tfidf_en_sv_dense: fit_transform use cache.\n",
      "2018-12-05 21:11:08,948 [INFO]   tfidf_en_sv_dense: transform use cache.\n",
      "2018-12-05 21:11:08,954 [INFO] Building features for lsa_500_en_sv...\n",
      "2018-12-05 21:11:08,955 [INFO]   lsa_500_en_sv: fit_transform use cache.\n",
      "2018-12-05 21:11:08,956 [INFO]   lsa_500_en_sv: transform use cache.\n",
      "2018-12-05 21:11:08,957 [INFO] Building features for lsa_1k_en_sv...\n",
      "2018-12-05 21:11:08,959 [INFO]   lsa_1k_en_sv: fit_transform use cache.\n",
      "2018-12-05 21:11:08,962 [INFO]   lsa_1k_en_sv: transform use cache.\n",
      "2018-12-05 21:11:08,963 [INFO] Building features for word2vec_en...\n",
      "2018-12-05 21:11:08,964 [INFO]   word2vec_en: fit_transform use cache.\n",
      "2018-12-05 21:11:08,965 [INFO]   word2vec_en: transform use cache.\n"
     ]
    }
   ],
   "source": [
    "build_features(['tfidf_en', 'tfidf_en_dense', 'lsa_500_en', 'lsa_1k_en'], fm)\n",
    "build_features(['tfidf_en_sv', 'tfidf_en_sv_dense', 'lsa_500_en_sv', 'lsa_1k_en_sv'], fm)\n",
    "# build_features(['tfidf_tiny', 'tfidf_tiny_dense', 'lsa_500_tiny'], fm)\n",
    "build_features(['word2vec_en'], fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/fm_cache-tfidf_en\n",
      "Saved data/fm_cache-lsa_500_en\n",
      "Saved data/fm_cache-lsa_1k_en\n",
      "Saved data/fm_cache-tfidf_en_sv\n",
      "Saved data/fm_cache-lsa_500_en_sv\n",
      "Saved data/fm_cache-lsa_1k_en_sv\n",
      "Saved data/fm_cache-word2vec_en\n"
     ]
    }
   ],
   "source": [
    "save_transform_cache('tfidf_en')\n",
    "save_transform_cache('lsa_500_en')\n",
    "save_transform_cache('lsa_1k_en')\n",
    "\n",
    "save_transform_cache('tfidf_en_sv')\n",
    "save_transform_cache('lsa_500_en_sv')\n",
    "save_transform_cache('lsa_1k_en_sv')\n",
    "\n",
    "# save_transform_cache('tfidf_tiny')\n",
    "# save_transform_cache('lsa_500_tiny')\n",
    "# save_transform_cache('lsa_1k_tiny')\n",
    "\n",
    "save_transform_cache('word2vec_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Very Basic TF-IDF + LDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact all feature models at once, so to avoid\n",
    "# classes being reloaded and causing save_model to fail\n",
    "from fgclassifier.baseline import Baseline, Dummy\n",
    "from fgclassifier.train import fm_cross_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:11:37,288 [INFO]   lsa_1k_en: fit_transform use cache.\n",
      "2018-12-05 21:11:47,336 [INFO]   lsa_1k_en: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:11:47,449 [INFO]  F1 Score: 0.4400537993260894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa_1k_en_SGD_Logistic final score: 0.4400537993260894\n"
     ]
    }
   ],
   "source": [
    "model = Baseline('SGD_Logistic', fm=fm['lsa_1k_en']['model'])\n",
    "# Always pass in the original features\n",
    "# the pipeline will take care of the cache\n",
    "model.fit(X_train, y_train)\n",
    "print(f'{model.name} final score:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic is much slower but performs not much better than Stochastic logistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:11:47,504 [INFO]   lsa_1k_en: fit_transform use cache.\n",
      "2018-12-05 21:11:56,691 [INFO]   lsa_1k_en: transform use cache.\n",
      "2018-12-05 21:11:56,836 [INFO]  F1 Score: 0.41832082945849275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa_1k_en_SGD_Huber final score: 0.41832082945849275\n"
     ]
    }
   ],
   "source": [
    "model = Baseline('SGD_Huber', fm=fm['lsa_1k_en']['model'])\n",
    "model.fit(X_train, y_train)\n",
    "print(f'{model.name} final score:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:11:56,905 [INFO]   lsa_1k_en: fit_transform use cache.\n",
      "2018-12-05 21:12:05,392 [INFO]   lsa_1k_en: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:12:05,499 [INFO]  F1 Score: 0.44261309492249856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa_1k_en_SGD_SVC final score: 0.44261309492249856\n"
     ]
    }
   ],
   "source": [
    "model = Baseline('SGD_SVC', fm=fm['lsa_1k_en']['model'])\n",
    "model.fit(X_train, y_train)\n",
    "print(f'{model.name} final score:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Baseline('Ridge', fm=fm['lsa_1k']['model'])\n",
    "# model.fit(X_train, y_train)\n",
    "# print(f'{model.name} final score:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the Best Feature + Classifier Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for all classifiers and feature builders\n",
    "train_avg_scores, train_scores = defaultdict(dict), defaultdict(dict)\n",
    "test_avg_scores, test_scores = defaultdict(dict), defaultdict(dict)\n",
    "test_time, train_time = defaultdict(dict), defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dummy(classifier=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fgclassifier import classifiers\n",
    "from fgclassifier.baseline import Dummy\n",
    "\n",
    "Dummy(classifiers.DummyStratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'fm_cache': fm,\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'results': {\n",
    "        'models': {},\n",
    "        'test': test_scores,\n",
    "        'test_time': test_time,\n",
    "        'test_avg': test_avg_scores,\n",
    "        'train': train_scores,\n",
    "        'train_time': train_time,\n",
    "        'train_avg': train_avg_scores\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# # We'd only need to run the dummy models on one feature model,\n",
    "# # as they do not care about the features\n",
    "fm_cross_check(\n",
    "    ['count_en', 'count_en_sv',\n",
    "     'tfidf_en', 'tfidf_en_sv'],\n",
    "    ['DummyStratified', 'DummyMostFrequent'],\n",
    "    model_cls=Dummy, **conf)\n",
    "\n",
    "# # Naive Bayes models cannot handle negative values, so we pass\n",
    "# # in only tfidf features\n",
    "fm_cross_check(\n",
    "    ['count_en', 'count_en_sv',\n",
    "     'tfidf_en', 'tfidf_en_sv'],\n",
    "    ['ComplementNB'], **conf)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only SGD methods can handle sparse matrix\n",
    "fm_cross_check(\n",
    "    [\n",
    "     'tfidf_en', 'lsa_500_en', 'lsa_1k_en',\n",
    "     'tfidf_en_sv', 'lsa_500_en_sv', 'lsa_1k_en_sv',\n",
    "     'count_en', 'count_en_sv',\n",
    "    ],\n",
    "    ['SGD_Logistic', 'SGD_SVC'], **conf)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All other models can run on many classifiers\n",
    "results = fm_cross_check(\n",
    "    ['lsa_500_en', 'lsa_1k_en',\n",
    "     'lsa_500_en_sv', 'lsa_1k_en_sv'],\n",
    "    ['LDA', 'Ridge'], **conf)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:20:02,487 [INFO] \n",
      "2018-12-05 21:20:02,488 [INFO] ============ Feature Model: word2vec_en ============\n",
      "2018-12-05 21:20:02,489 [INFO] \n",
      "2018-12-05 21:20:02,489 [INFO] Train for word2vec_en -> SGD_Logistic...\n",
      "2018-12-05 21:20:02,490 [INFO]   word2vec_en: fit_transform use cache.\n",
      "2018-12-05 21:20:40,366 [INFO]   word2vec_en: transform use cache.\n",
      "2018-12-05 21:20:40,414 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:20:40,414 [INFO] 【word2vec_en -> SGD_Logistic】 Train: 0.3425, Test: 0.3176\n",
      "2018-12-05 21:20:40,415 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:20:40,415 [INFO] Train for word2vec_en -> SGD_SVC...\n",
      "2018-12-05 21:20:40,416 [INFO]   word2vec_en: fit_transform use cache.\n",
      "2018-12-05 21:20:44,228 [INFO]   word2vec_en: transform use cache.\n",
      "2018-12-05 21:20:44,458 [INFO]   word2vec_en: transform use cache.\n",
      "2018-12-05 21:20:44,500 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:20:44,501 [INFO] 【word2vec_en -> SGD_SVC】 Train: 0.3682, Test: 0.3409\n",
      "2018-12-05 21:20:44,501 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:20:44,502 [INFO] Train for word2vec_en -> LDA...\n",
      "2018-12-05 21:20:44,503 [INFO]   word2vec_en: fit_transform use cache.\n",
      "2018-12-05 21:20:54,444 [INFO]   word2vec_en: transform use cache.\n",
      "2018-12-05 21:20:54,656 [INFO]   word2vec_en: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:20:54,700 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:20:54,700 [INFO] 【word2vec_en -> LDA】 Train: 0.5124, Test: 0.4126\n",
      "2018-12-05 21:20:54,701 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:20:54,701 [INFO] Train for word2vec_en -> Ridge...\n",
      "2018-12-05 21:20:54,702 [INFO]   word2vec_en: fit_transform use cache.\n",
      "2018-12-05 21:21:03,334 [INFO]   word2vec_en: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:21:03,532 [INFO]   word2vec_en: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:21:03,581 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:21:03,581 [INFO] 【word2vec_en -> Ridge】 Train: 0.3571, Test: 0.3405\n",
      "2018-12-05 21:21:03,582 [INFO] -------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = fm_cross_check(\n",
    "    ['word2vec_en'],\n",
    "    ['SGD_Logistic', 'SGD_SVC', 'LDA', 'Ridge'], **conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:21:03,622 [INFO] \n",
      "2018-12-05 21:21:03,623 [INFO] ============ Feature Model: tfidf_en_sv_dense ============\n",
      "2018-12-05 21:21:03,624 [INFO] \n",
      "2018-12-05 21:21:03,624 [INFO] Train for tfidf_en_sv_dense -> LDA...\n",
      "2018-12-05 21:21:03,625 [INFO]   tfidf_en_sv_dense: fit_transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "2018-12-05 21:24:16,191 [INFO]   tfidf_en_sv: transform use cache.\n",
      "2018-12-05 21:24:16,757 [INFO]   tfidf_en_sv_dense: transform use cache.\n",
      "2018-12-05 21:24:16,897 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:24:16,898 [INFO] 【tfidf_en_sv_dense -> LDA】 Train: 0.8396, Test: 0.4483\n",
      "2018-12-05 21:24:16,899 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:24:16,900 [INFO] Train for tfidf_en_sv_dense -> Ridge...\n",
      "2018-12-05 21:24:16,900 [INFO]   tfidf_en_sv_dense: fit_transform use cache.\n",
      "2018-12-05 21:27:35,654 [INFO]   tfidf_en_sv_dense: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:27:36,442 [INFO]   tfidf_en_sv_dense: transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-12-05 21:27:36,616 [INFO] -------------------------------------------------------\n",
      "2018-12-05 21:27:36,617 [INFO] 【tfidf_en_sv_dense -> Ridge】 Train: 0.4138, Test: 0.3576\n",
      "2018-12-05 21:27:36,618 [INFO] -------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = fm_cross_check(\n",
    "    ['tfidf_en_sv_dense'],\n",
    "    ['LDA', 'Ridge'], **conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 21:27:36,680 [INFO] \n",
      "2018-12-05 21:27:36,681 [INFO] ============ Feature Model: tfidf_en_dense ============\n",
      "2018-12-05 21:27:36,682 [INFO] \n",
      "2018-12-05 21:27:36,683 [INFO] Train for tfidf_en_dense -> LDA...\n",
      "2018-12-05 21:27:36,684 [INFO]   tfidf_en_dense: fit_transform use cache.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# This is very slow ...\n",
    "results = fm_cross_check(\n",
    "    ['tfidf_en_dense'],\n",
    "    ['LDA', 'Ridge'], **conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Boosting is too damn slow...\n",
    "# results = fm_cross_check(\n",
    "#     ['lsa_500',\n",
    "#      'lsa_500_sv',\n",
    "#      'lsa_500_tiny',\n",
    "#      'lsa_1k',\n",
    "#      'word2vec',\n",
    "#     ],\n",
    "#     ['XGB', 'AdaBoost', 'GradientBoost'], **conf)\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dense(obj):\n",
    "    obj = obj.copy()\n",
    "    obj['tfidf_en'] = {\n",
    "        **obj['tfidf_en_dense'],\n",
    "        **obj['tfidf_en']\n",
    "    }\n",
    "    obj['tfidf_en_sv'] = {\n",
    "        **obj['tfidf_en_sv_dense'],\n",
    "        **obj['tfidf_en_sv']\n",
    "    }\n",
    "    del obj['tfidf_en_dense']\n",
    "    del obj['tfidf_en_sv_dense']\n",
    "    return obj\n",
    "\n",
    "def extract_scores(scores, avg_scores):\n",
    "    scores = merge_dense(scores)\n",
    "    avg_scores = merge_dense(avg_scores)\n",
    "    rows = {}\n",
    "    for fm_name in scores:\n",
    "        for clf_name in avg_scores[fm_name]:\n",
    "            key = f'{fm_name}.{clf_name}'\n",
    "            rows[key] = [avg_scores[fm_name][clf_name], *scores[fm_name][clf_name]]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.index = ['average', *y_train.columns]\n",
    "    return df.T.sort_values('average', ascending=False)\n",
    "\n",
    "df_train = extract_scores(train_scores, train_avg_scores)\n",
    "df_test = extract_scores(test_scores, test_avg_scores)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "results = conf['results'].copy()\n",
    "del results['models']  # don't save models (which are huuuuge)\n",
    "joblib.dump(results, 'data/model-selection-en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important columns\n",
    "imp_cols = [\n",
    "    'count_en', 'count_en_sv',\n",
    "    'tfidf_en', 'tfidf_en_sv',\n",
    "    'lsa_500_en', 'lsa_500_en_sv',\n",
    "    'lsa_1k_en', 'lsa_1k_en_sv',\n",
    "    'word2vec_en']\n",
    "\n",
    "def extract_avg_scores(scores):\n",
    "    scores = merge_dense(scores)\n",
    "    df = pd.DataFrame(scores)\n",
    "    df['avg'] = df.mean(axis=1, skipna=True)\n",
    "    df = df.T\n",
    "    df['avg'] = df.mean(axis=1, skipna=True)\n",
    "    df = df.T\n",
    "    df = df.sort_values(by='avg', axis=1, ascending=False)\n",
    "    df = df.sort_values(by='avg', ascending=False)\n",
    "    df = df.drop(['avg'], axis=1)\n",
    "    df = df.drop(['avg'], axis=0)\n",
    "    return df[imp_cols]\n",
    "\n",
    "df_train_avg = extract_avg_scores(train_avg_scores)\n",
    "df_test_avg = extract_avg_scores(test_avg_scores)\n",
    "df_test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_running_time(times):\n",
    "    df = pd.DataFrame(times)\n",
    "    df['avg'] = df.mean(axis=1, skipna=True)\n",
    "    df = df.T\n",
    "    df['avg'] = df.mean(axis=1, skipna=True)\n",
    "    df = df.T\n",
    "    df = df.sort_values(by='avg', axis=1, ascending=True)\n",
    "    df = df.sort_values(by='avg', ascending=True)\n",
    "    df = df.drop(['avg'], axis=1)\n",
    "    df = df.drop(['avg'], axis=0)\n",
    "    return df[imp_cols]\n",
    "    \n",
    "df_train_time = extract_running_time(train_time)\n",
    "df_test_time = extract_running_time(test_time)\n",
    "df_train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_test.drop([\n",
    "    'tfidf.DummyStratified',\n",
    "    'tfidf.DummyMostFrequent',\n",
    "    'tfidf_sv.DummyStratified',\n",
    "    'tfidf_sv.DummyMostFrequent',\n",
    "    'count_sv.DummyStratified',\n",
    "    'count_sv.DummyMostFrequent',\n",
    "]).T.drop(['average']).boxplot(\n",
    "    figsize=(18, 6), rot=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = conf['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fgclassifier.utils import save_model\n",
    "\n",
    "def clear_cache(model):\n",
    "    if hasattr(model, 'steps'):\n",
    "        for (name, step) in model.steps:\n",
    "            clear_cache(step)\n",
    "    if hasattr(model, 'cache'):\n",
    "        model.cache = None\n",
    "    return model\n",
    "\n",
    "for name, model in results['models'].items():\n",
    "    # skip unimportant models\n",
    "    if 'QDA' in name:\n",
    "        continue\n",
    "    clear_cache(model)\n",
    "    save_model(model)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = results['models']['lsa_500_sv_SGD_SVC'].named_steps.lsa_500_sv.named_steps.lsa_500_sv\n",
    "x.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- `ComplementNB` performs much better than a simple MultinomialNB, because our class labels are mostly unbalanced.\n",
    "- `LatentDirichletAllocation` topics as features are not suitable for our classification problem, as features are often collinear. They often fare no better than the dummy classifier where we simply return the most frequent labels.\n",
    "- LSA (Latent Semantic Analysis, Tfidf + SVD) shows a much more promising outlook, especially when combined with Linear Discriminant Analysis or SVC.\n",
    "- Find the right vocabulary (min_df, ngram range, max_features) is crucial. Throw away noises early often outperforms running dimension reduction later.\n",
    "- Basically SVD makes each feature (component) more indendent with each other, making LDA and SVC easier to come up with good fittings.\n",
    "- Tree based models are not particularly useful. But the results may be different had we tuned the tree structure more.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Required:\n",
    "\n",
    "- Tune hyperparamters for `ComplementNB`, `TruncatedSVD`, `LinearDiscriminantAnalysis` and `SVC`/`LinearSVC`. Try different kernel functions.\n",
    "- Try over-/under-sampling since most of our classes are imbalanced. [Possible solution](https://imbalanced-learn.org/)\n",
    "- Test some boosting methods, especially [xgboost](https://xgboost.readthedocs.io/en/latest/).\n",
    "- Test word embedding as features.\n",
    "\n",
    "Optional:\n",
    "\n",
    "- Possibly use different classifier for different labels.\n",
    "- Test two step predictions: first run binary prediction for \"mentioned\" vs \"not mentioned\", i.e., -2 vs (-1, 0, 1), then predict (-1, 0, 1).\n",
    "    - This could happen as either [ClassifierChain](https://scikit-learn.org/stable/modules/multiclass.html#classifierchain) or separate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = results['models']['lsa_500_en_LDA']\n",
    "# print(X_test[0:1].shape)\n",
    "# probas = model.predict_proba(X_test[0:1])\n",
    "# probas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
