{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('jieba').setLevel(logging.WARN)\n",
    "logging.getLogger('fgclassifier').setLevel(logging.INFO)\n",
    "\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the methods to do tag highlighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 22:16:10,333 [INFO] Reading /opt/storage/english_train.csv..\n",
      "2018-12-02 22:16:10,511 [INFO] Reading /opt/storage/train/sentiment_analysis_trainingset.csv.tokenized.csv..\n",
      "2018-12-02 22:16:12,807 [INFO] Reading /opt/storage/train/sentiment_analysis_trainingset.csv..\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fgclassifier.utils import read_data, get_dataset\n",
    "\n",
    "df_en = get_dataset('train_en')\n",
    "df_zh = get_dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First of all, the service, I asked the price to the waiter to see if this is 98, she said that it was then when she came to collect the money, but 108, asked her why she could not answer the question, then another person came over to collect the money and then gave it to me. 10 cheaper. Sure enough, Panyu is not a good thing, but it is actually directed. The cooperation between the waiters is poor, the wrong order is made, and the dish is messed up. Ok, say fried chicken. I ordered bt spicy and sweet. When the chicken comes up, I will sip it. Isn't it just a dish of fried sauce and sweet sauce on the fried chicken? Ok, I will bear it first, try it. Um... Too much powder, and it’s too fragile, the chicken inside is a bit dry. And I have eaten one of the chickens with a strange taste. I dare not say that using fresh chicken, it should be an old chilled chicken, it should still be 82 years. Potato chips... um... it’s just a pack of 3 in the supermarket. Potato wedges, good oil, no return bombs? It would be better if the bombing forced out the oil. Cannonball soda, very good, a big cup, very good, but sell 48 cups, no one should buy it alone. Overall... Everyone wants to eat fried chicken or work hard to go to the distant road.\n",
      "吼吼吼，萌死人的棒棒糖，中了大众点评的霸王餐，太可爱了。一直就好奇这个棒棒糖是怎么个东西，大众点评给了我这个土老冒一个见识的机会。看介绍棒棒糖是用德国糖做的，不会很甜，中间的照片是糯米的，能食用，真是太高端大气上档次了，还可以买蝴蝶结扎口，送人可以买礼盒。我是先打的卖家电话，加了微信，给卖家传的照片。等了几天，卖家就告诉我可以取货了，去大官屯那取的。虽然连卖家的面都没见到，但是还是谢谢卖家送我这么可爱的东西，太喜欢了，这哪舍得吃啊。\n"
     ]
    }
   ],
   "source": [
    "text_en = df_en['content_raw'][0]\n",
    "text_zh = df_zh['content_raw'][0]\n",
    "print(text_en)\n",
    "print(text_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02571608877754472"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snownlp import SnowNLPNLP\n",
    "\n",
    "SnowNLP('实在是太恶心了').sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_zh = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吼吼吼 0.5834396182746914\n",
      "吼吼 NN\n",
      "吼 SFN\n",
      "萌死人的棒棒糖 0.9586401818140926\n",
      "萌死 NNP\n",
      "人 NN\n",
      "的 DEC\n",
      "棒棒糖 NN\n",
      "中了大众点评的霸王餐 0.6498323781194285\n",
      "中 NN\n",
      "了 AS\n",
      "大众 NN\n",
      "点评 NN\n",
      "的 DEC\n",
      "霸王餐 NN\n",
      "太可爱了 0.6663463298769546\n",
      "太 RB\n",
      "可爱 JJ\n",
      "了 UH\n",
      "一直就好奇这个棒棒糖是怎么个东西 0.997882635536866\n",
      "一直 RB\n",
      "就 RB\n",
      "好奇 JJ\n",
      "这个 DT\n",
      "棒棒糖 NN\n",
      "是 VC\n",
      "怎么 RB\n",
      "个 NNB\n",
      "东西 NN\n",
      "大众点评给了我这个土老冒一个见识的机会 0.752802797601812\n",
      "大众 NN\n",
      "点评 VV\n",
      "给 VV\n",
      "了 AS\n",
      "我 PRP\n",
      "这个 DT\n",
      "土老冒 NNP\n",
      "一个 RB\n",
      "见识 VV\n",
      "的 DEC\n",
      "机会 NN\n",
      "看介绍棒棒糖是用德国糖做的 0.992540777499147\n",
      "看 VV\n",
      "介绍 VV\n",
      "棒棒糖 NN\n",
      "是 VC\n",
      "用 VV\n",
      "德国 NNP\n",
      "糖 NN\n",
      "做 VV\n",
      "的 UH\n",
      "不会很甜 0.7847423585226886\n",
      "不会 MD\n",
      "很甜 JJ\n",
      "中间的照片是糯米的 0.6028834542837995\n",
      "中间 NN\n",
      "的 DEC\n",
      "照片 NN\n",
      "是 VC\n",
      "糯米 VV\n",
      "的 UH\n",
      "能食用 0.5833333333333336\n",
      "能 MD\n",
      "食用 VV\n",
      "真是太高端大气上档次了 0.9853961767368831\n",
      "真是太 NN\n",
      "高端 JJ\n",
      "大气 NN\n",
      "上档次 VV\n",
      "了 UH\n",
      "还可以买蝴蝶结扎口 0.7683977089874504\n",
      "还 RB\n",
      "可以 MD\n",
      "买 VV\n",
      "蝴蝶结 NN\n",
      "扎口 JJ\n",
      "送人可以买礼盒 0.5572971562741176\n",
      "送人 VV\n",
      "可以 MD\n",
      "买 VV\n",
      "礼盒 NN\n",
      "我是先打的卖家电话 0.19276543154215775\n",
      "我 PRP\n",
      "是 VC\n",
      "先 RB\n",
      "打 VV\n",
      "的 DEC\n",
      "卖家 JJ\n",
      "电话 NN\n",
      "加了微信 0.638246931497096\n",
      "加 VV\n",
      "了 AS\n",
      "微信 NNP\n",
      "给卖家传的照片 0.5190102704406371\n",
      "给 VV\n",
      "卖家 NN\n",
      "传 SFV\n",
      "的 DEC\n",
      "照片 NN\n",
      "等了几天 0.38121739130434773\n",
      "等 NN\n",
      "了 AS\n",
      "几天 NN\n",
      "卖家就告诉我可以取货了 0.21622707986907486\n",
      "卖家 NN\n",
      "就 RB\n",
      "告诉 VV\n",
      "我 PRP\n",
      "可以 MD\n",
      "取货 VV\n",
      "了 AS\n",
      "去大官屯那取的 0.46073148841656275\n",
      "去 VV\n",
      "大官 NN\n",
      "屯 SFN\n",
      "那取 NNP\n",
      "的 DEC\n",
      "虽然连卖家的面都没见到 0.4392861422856016\n",
      "虽然 IN\n",
      "连 RB\n",
      "卖家 JJ\n",
      "的 DEC\n",
      "面 NN\n",
      "都 RB\n",
      "没 RB\n",
      "见到 VV\n",
      "但是还是谢谢卖家送我这么可爱的东西 0.8263142383380964\n",
      "但是 RB\n",
      "还是 RB\n",
      "谢谢 VV\n",
      "卖家 NN\n",
      "送 VV\n",
      "我 PRP\n",
      "这么 RB\n",
      "可爱 JJ\n",
      "的 DEC\n",
      "东西 NN\n",
      "太喜欢了 0.5233886390303538\n",
      "太 RB\n",
      "喜欢 VV\n",
      "了 AS\n",
      "这哪舍得吃啊 0.5282136447676961\n",
      "这 DT\n",
      "哪 WP\n",
      "舍得吃 NNP\n",
      "啊 .\n"
     ]
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "\n",
    "doc = SnowNLP(text_zh)\n",
    "for sent in doc.sentences:\n",
    "    print(sent, SnowNLP(sent).sentiments)\n",
    "    for word in nlp_zh(sent):\n",
    "        print(word, word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hahah!!! What are you Doing.? Aba Whats..\n",
      "Hahah!!! 0.5\n",
      " What are you Doing.? 0.4737672181921908\n",
      " Aba Whats.. 0.4737672181921908\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "RE_SENTENCE = re.compile(r'.*?[。….？！?!；~～]+') \n",
    "RE_BLANK_AND_MARK = re.compile(r'\\s+([。….？！?!；~～])')\n",
    "\n",
    "text = 'Hahah! ! ! What are you Doing.? Aba Whats..'\n",
    "\n",
    "text = RE_BLANK_AND_MARK.sub(r'\\1', text)\n",
    "print(text)\n",
    "\n",
    "for sent in RE_SENTENCE.findall(text):\n",
    "    sent = sent\n",
    "    print(sent, SnowNLP(sent).sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First of all, the service, I asked the price to the waiter to see if this is 98, she said that it was then when she came to collect the money, but 108, asked her why she could not answer the question, then another person came over to collect the money and then gave it to me.\n",
      "10 cheaper.\n",
      "Sure enough, Panyu is not a good thing, but it is actually directed.\n",
      "The cooperation between the waiters is poor, the wrong order is made, and the dish is messed up.\n",
      "Ok, say fried chicken.\n",
      "I ordered bt spicy and sweet.\n",
      "When the chicken comes up, I will sip it.\n",
      "Isn't it just a dish of fried sauce and sweet sauce on the fried chicken?\n",
      "Ok, I will bear it first, try it.\n",
      "Um... Too much powder, and it’s too fragile, the chicken inside is a bit dry.\n",
      "And I have eaten one of the chickens with a strange taste.\n",
      "I dare not say that using fresh chicken, it should be an old chilled chicken, it should still be 82 years.\n",
      "Potato chips... um... it’s just a pack of 3 in the supermarket.\n",
      "Potato wedges, good oil, no return bombs?\n",
      "It would be better if the bombing forced out the oil.\n",
      "Cannonball soda, very good, a big cup, very good, but sell 48 cups, no one should buy it alone.\n",
      "Overall... Everyone wants to eat fried chicken or work hard to go to the distant road.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text_en)\n",
    "for sent in blob.sentences:\n",
    "    print(sent.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "很高兴和小伙伴们一起品鉴蜀老头烤鱼，此店位于花园路，在小天鹅东来顺的西边，店内环境古香古色，各种装饰道具让人有一种身在蜀中的韵味。<span class=\"positive\">包间里的家具</span>都是原木色的实木桌櫈，记得小时候不管教室里还是家里都是<span class=\"positive\">这样造型的桌椅</span>让我找到了童年的回忆，很温馨舒服。菜品更是丰富多彩，薯香辣翅中是最受欢迎的一道菜虽然看起来很多辣椒但是鸡翅里只融入了辣椒的香味还有烤得<span class=\"positive\">适中的地瓜条味道</span>很不错。葱香清江鱼也是当场很卖座得一道菜，甜咸可口，酱汁浓郁。香辣清江鱼店家使用了五月梅花椒，是即带麻味又带<span class=\"positive\">香味的花椒</span>再加上四川绵阳的干辣椒，真是红红火火满锅红。口味自然鲜辣无比，爱吃辣的同学一定不能错过。还有<span class=\"positive\">干锅辣</span>鸭头，干锅牛蛙口味都不错而且量大实惠。<span class=\"positive\">唯一的缺点</span>就是所有菜品口味有些重，而且越吃会越咸如果能改善一下这方面那就更完美了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fgclassifier.visualizer.highlight import highlight_noun_chunks\n",
    "\n",
    "text_zh = \"\"\"\n",
    "很高兴和小伙伴们一起品鉴蜀老头烤鱼，此店位于花园路，在小天鹅东来顺的西边，店内环境古香古色，各种装饰道具让人有一种身在蜀中的韵味。包间里的家具都是原木色的实木桌櫈，记得小时候不管教室里还是家里都是这样造型的桌椅让我找到了童年的回忆，很温馨舒服。菜品更是丰富多彩，薯香辣翅中是最受欢迎的一道菜虽然看起来很多辣椒但是鸡翅里只融入了辣椒的香味还有烤得适中的地瓜条味道很不错。葱香清江鱼也是当场很卖座得一道菜，甜咸可口，酱汁浓郁。香辣清江鱼店家使用了五月梅花椒，是即带麻味又带香味的花椒再加上四川绵阳的干辣椒，真是红红火火满锅红。口味自然鲜辣无比，爱吃辣的同学一定不能错过。还有干锅辣鸭头，干锅牛蛙口味都不错而且量大实惠。唯一的缺点就是所有菜品口味有些重，而且越吃会越咸如果能改善一下这方面那就更完美了。\n",
    "\"\"\"\n",
    "\n",
    "print(highlight_noun_chunks(text_zh, 'zh'))\n",
    "# print()\n",
    "# print(highlight_noun_chunks(text_en, 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吼吼 \t 吼吼 NN appos\n",
      "吼 \t 吼 SFN nmod\n",
      "， \t 吼吼 NNP nmod\n",
      "萌死 \t 萌死 IN case:suff\n",
      "人 \t 吼吼 NN det\n",
      "的 \t 的 DEC case:dec\n",
      "棒棒糖 \t 棒棒糖 NN nmod\n",
      "， \t 吼吼 RB advmod\n",
      "中 \t 中 VV acl\n",
      "了 \t 了 AS case:aspect\n",
      "大众 \t 大众 NN nsubj\n",
      "点评 \t 大众 NN acl:relcl\n",
      "的 \t 的 DEC mark:relcl\n",
      "霸王餐 \t 大众 NN nmod\n",
      "， \t 吼吼 RB nsubj\n",
      "太 \t 太 RB advmod\n",
      "可爱 \t 吼吼 JJ ROOT\n",
      "了 \t 了 AS case:aspect\n",
      "。 \t 。 FW obj\n",
      "棒棒糖\n",
      "大众\n"
     ]
    }
   ],
   "source": [
    "from fgclassifier.visualizer.highlight import spacy_load\n",
    "from spacy.lang.zh import TAG_MAP\n",
    "\n",
    "\n",
    "def zh_noun_chunks_iterator(obj):\n",
    "    \"\"\"\n",
    "    Iterate Chinse noun chunks\n",
    "    \"\"\"\n",
    "    labels = ['nmod', 'punct', 'obj', 'nsubj',\n",
    "              'dobj', 'nsubjpass', 'pcomp', 'pobj', 'dative',\n",
    "              'attr', 'ROOT']\n",
    "\n",
    "    doc = obj.doc # Ensure works on both Doc and Span.\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add('conj')\n",
    "    np_label = doc.vocab.strings.add('NP')\n",
    "    \n",
    "    seen = set()\n",
    "    exclude = set(['，', ','])  # always exclude 「，」\n",
    "    for i, word in enumerate(obj):\n",
    "        print(word, '\\t', word.left_edge, word.tag_, word.dep_)\n",
    "        if word.tag_ not in ('NNP', 'NN', 'RB'):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.i in seen or word.text in exclude:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            # print([w for w in word.subtree])\n",
    "            if any((w.i in seen or w.text in exclude) for w in word.subtree):\n",
    "                continue\n",
    "            seen.update(j for j in range(word.left_edge.i, word.i+1))\n",
    "            yield word.left_edge.i, word.i+1, np_label\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                if any(w.i in seen for w in word.subtree):\n",
    "                    continue\n",
    "                seen.update(j for j in range(word.left_edge.i, word.i+1))\n",
    "                yield word.left_edge.i, word.i+1, np_label\n",
    "\n",
    "def zh_noun_chunks(doc):\n",
    "    doc.noun_chunks_iterator = zh_noun_chunks_iterator\n",
    "    return doc.noun_chunks\n",
    "    \n",
    "nlp = spacy_load('zh')\n",
    "doc = nlp('吼吼吼，萌死人的棒棒糖，中了大众点评的霸王餐，太可爱了。')\n",
    "# doc = nlp('一直就好奇这个棒棒糖是怎么个东西，大众点评给了我这个土老冒一个见识的机会。')\n",
    "# doc = nlp('看介绍棒棒糖是用德国糖做的，不会很甜，中间的照片是糯米的，能食用，真是太高端大气上档次了，还可以买蝴蝶结扎口，送人可以买礼盒。')\n",
    "# doc = nlp('虽然连卖家的面都没见到，但是还是谢谢卖家送我这么可爱的东西，太喜欢了，这哪舍得吃啊。')\n",
    "\n",
    "for x in zh_noun_chunks(doc):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=\"sentence\">I want to eat <span class=\"neutral\">grilled fish</span>, I found out that there is a fish near <span class=\"neutral\">the subway station</span>, and I think that there is no fish or fish in Nanjing.</span><span class=\"sentence\">I am going to eat this.</span><span class=\"sentence\"><span class=\"positive\">[Location</span>] It’s really good for a person who came to this square for <span class=\"positive\">the first time</span>.</span><span class=\"sentence\">Difficult to find, I don\\'t know if I still divide it into <span class=\"negative\">the east</span> and <span class=\"negative\">west districts</span>.</span><span class=\"sentence\">I found it in <span class=\"negative\">the inside</span> for <span class=\"negative\">half an hour</span>.</span><span class=\"sentence\">It was next to <span class=\"neutral\">the hottest Guimanyu</span>.</span><span class=\"sentence\"><span class=\"positive\">[Environment</span>] is <span class=\"positive\">the kind</span> of lighting that is dark and iron-isolated.</span><span class=\"sentence\">Quite a lot~</span>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fgclassifier.visualizer.highlight import highlight_noun_chunks\n",
    "\n",
    "text = \"\"\"\n",
    "I want to eat grilled fish, I found out that there is a fish near the subway station, and I think that there is no fish or fish in Nanjing. I am going to eat this. [Location] It\\u2019s really good for a person who came to this square for the first time. Difficult to find, I don't know if I still divide it into the east and west districts. I found it in the inside for half an hour. It was next to the hottest Guimanyu. [Environment] is the kind of lighting that is dark and iron-isolated. Quite a lot ~ [Grilled fish] Two people only ordered a tomato grilled fish, 3 kg of Qingjiang fish, added potatoes,\n",
    "\"\"\"\n",
    "\n",
    "highlight_noun_chunks(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
