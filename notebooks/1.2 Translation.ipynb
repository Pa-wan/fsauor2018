{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset is in Chinese, to facilitate communication with team members and instructors, we are creating a subset of English translations using Google Translate API.\n",
    "\n",
    "If you want to run the following code yourself, follow the instructions [here](https://cloud.google.com/translate/docs/quickstart-client-libraries#client-libraries-install-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 02:01:38,903 [INFO] Reading /opt/storage/train/sentiment_analysis_trainingset.csv..\n"
     ]
    }
   ],
   "source": [
    "from config import valid_data_path, train_data_path, testa_data_path\n",
    "from fgclassifier import read_csv\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "df_train_raw = read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015\n"
     ]
    }
   ],
   "source": [
    "from config import data_root \n",
    "\n",
    "cache_path = f'{data_root}/train/en.pkl'\n",
    "try:\n",
    "    translations = joblib.load(cache_path)\n",
    "    print(len(translations))\n",
    "except:\n",
    "    translations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10923/12000 [18:03<21:33,  1.20s/it]   "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "from google.cloud import translate\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "# All available credentials\n",
    "credentials = glob.glob('../data/google-cloud/*.json')\n",
    "\n",
    "# Use multiple credentials to bypass rate limit\n",
    "clients = []\n",
    "for credential in credentials:\n",
    "    print(credential)\n",
    "    clients.append(translate.Client.from_service_account_json(credential))\n",
    "\n",
    "df = df_train_raw.copy().iloc[0:12000,:]\n",
    "contents = [x.strip('\"') for x in df['content']]\n",
    "n_client = len(clients)\n",
    "n_records = df.shape[0]\n",
    "\n",
    "client_ok = [True for _ in clients]\n",
    "\n",
    "\n",
    "def get_client(i):\n",
    "    c = 0\n",
    "    while not client_ok[i % n_client] and c < n_client:\n",
    "        c += 1\n",
    "        i += 1\n",
    "    i = i % n_client\n",
    "    client = clients[i] if c < n_client else None\n",
    "    return i, client\n",
    "\n",
    "failed = []\n",
    "\n",
    "clear_output()\n",
    "pbar = tqdm(total=n_records)\n",
    "queue = list(range(n_records))\n",
    "n_failed = 0\n",
    "\n",
    "while len(queue) and n_failed < n_client:\n",
    "    i = queue.pop(0)\n",
    "    if i not in translations:\n",
    "        start_time = time.time()\n",
    "        client_idx, client = get_client(i)\n",
    "        if not client:\n",
    "            raise RuntimeError('No Available Client.')\n",
    "        try:\n",
    "            translation = client.translate(contents[i],\n",
    "                target_language='en', source_language='zh')\n",
    "            translations[i] = translation['translatedText']\n",
    "        except Exception as e:\n",
    "            # print(client_idx + 1, e)\n",
    "            client_ok[client_idx] = False\n",
    "            queue.append(i)\n",
    "            n_failed += 1\n",
    "            continue\n",
    "        end_time = time.time()\n",
    "        # If finished within 1 second, wait...\n",
    "        if end_time < start_time + 0.5:\n",
    "            time.sleep(start_time + 0.5 - end_time)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/storage/train/en.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(translations, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace content with translation, and replace apostrophe \n",
    "df['content'] = [x.replace('&#39;', \"'\") for x in pd.Series(translations).sort_index()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idx = np.random.randint(0, 10000)\n",
    "print(df_train['content'][idx])\n",
    "print(df['content'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data obtained by Google Translating to English\n",
    "df.to_csv('data/english.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to a training set and a hold-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 15:54:43,093 [INFO] Reading data/english.csv..\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from fgclassifier.utils import read_csv\n",
    "\n",
    "df = read_csv('data/english.csv')\n",
    "df_train = df.sample(frac=0.8, random_state=42)\n",
    "df_valid = df.drop(df_train.index)\n",
    "\n",
    "df_train.to_csv('data/english_train.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "df_valid.to_csv('data/english_valid.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
