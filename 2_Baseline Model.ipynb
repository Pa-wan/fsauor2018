{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('jieba').setLevel(logging.WARN)\n",
    "logging.getLogger('fgclassifier').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-19 23:06:52,956 [INFO] Reading data/english.csv..\n",
      "2018-11-19 23:06:53,150 [INFO] Building features for count...\n",
      "2018-11-19 23:06:58,923 [INFO] Building features for tfidf...\n",
      "2018-11-19 23:06:58,924 [INFO]   count: fit_transform use cache.\n",
      "2018-11-19 23:06:59,037 [INFO]   count: transform use cache.\n",
      "2018-11-19 23:06:59,051 [INFO] Building features for lsa_100...\n",
      "2018-11-19 23:06:59,053 [INFO]   tfidf: fit_transform use cache.\n",
      "2018-11-19 23:07:00,520 [INFO]   tfidf: transform use cache.\n",
      "2018-11-19 23:07:00,570 [INFO] Building features for lsa_1k...\n",
      "2018-11-19 23:07:00,571 [INFO]   tfidf: fit_transform use cache.\n",
      "2018-11-19 23:07:16,477 [INFO]   tfidf: transform use cache.\n",
      "2018-11-19 23:07:16,820 [INFO] Building features for lda_count_100...\n",
      "2018-11-19 23:07:16,821 [INFO]   count: fit_transform use cache.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from fgclassifier.features import FeaturePipeline, logger\n",
    "from fgclassifier.utils import read_data\n",
    "\n",
    "X, y = read_data('data/english.csv', seg_words=False, sample_n=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# feature models with dependency specs\n",
    "fm_spec = {\n",
    "   'count': CountVectorizer(ngram_range=(1, 4), min_df=0.01, max_df=0.99),\n",
    "   'tfidf': ['count', TfidfTransformer],\n",
    "   'lsa_100': ['tfidf', TruncatedSVD(n_components=100)],\n",
    "   'lsa_1k': ['tfidf', TruncatedSVD(n_components=1000)],\n",
    "   'lda_count_100': ['count', LatentDirichletAllocation(n_components=100)],\n",
    "   'lda_count_200': ['count', LatentDirichletAllocation(n_components=200)],\n",
    "   'lda_tfidf_100': ['tfidf', LatentDirichletAllocation(n_components=100)],\n",
    "   'lda_tfidf_200': ['tfidf', LatentDirichletAllocation(n_components=200)],\n",
    "    \n",
    "    # small vocabulary (removed more stop words)\n",
    "   'count_sv': [CountVectorizer(ngram_range=(1, 4), min_df=0.01, max_df=0.85)],\n",
    "   'tfidf_sv': ['count_sv', TfidfTransformer],\n",
    "   'lsa_100_sv': ['tfidf_sv', TruncatedSVD(n_components=100)],\n",
    "   'lsa_1k_sv': ['tfidf_sv', TruncatedSVD(n_components=1000)],\n",
    "   'lda_count_100_sv': ['count_sv', LatentDirichletAllocation(n_components=100)],\n",
    "   'lda_count_200_sv': ['count_sv', LatentDirichletAllocation(n_components=200)],\n",
    "   'lda_tfidf_100_sv': ['tfidf_sv', LatentDirichletAllocation(n_components=100)],\n",
    "   'lda_tfidf_200_sv': ['tfidf_sv', LatentDirichletAllocation(n_components=200)],\n",
    "}\n",
    "\n",
    "# Cache trained fetures, we make this cache object\n",
    "# so different steps can reuse previously trained features\n",
    "fm = defaultdict(dict)\n",
    "\n",
    "for name in fm_spec.keys():\n",
    "    logger.info(f'Building features for {name}...')\n",
    "    model = FeaturePipeline(name, spec=fm_spec, cache=fm)\n",
    "    fmx_train[name] = model.fit_transform(X_train)\n",
    "    fmx_test[name] = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exam the quality of the top terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Vocab size: %s' % len(fm['count'].vocabulary_))\n",
    "print('Most common words: \\n')\n",
    "print('\\n'.join(['%s \\t %s' % (k, v) for k, v in\n",
    "                 Counter(fm['count'].vocabulary_).most_common()[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Very Basic TF-IDF + LDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fgclassifier.baseline import Baseline\n",
    "from fgclassifier.classifiers import LDA\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "model = Baseline(classifier=LDA)\n",
    "model.fit(fmx_train[''], y_train)\n",
    "model.score(fmx_test[''], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the Best Feature + Classifier Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for all classifiers and feature builders\n",
    "models = defaultdict(dict)\n",
    "all_avg_scores, all_scores = defaultdict(dict), defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from fgclassifier.baseline import logger\n",
    "from fgclassifier.features import FeaturePipeline, SparseToSense\n",
    "from fgclassifier.features import LatentDirichletAllocation, SVD\n",
    "from fgclassifier import classifiers\n",
    "\n",
    "# Naive Bayes models cannot handle negative values, so we pass\n",
    "# in raw tf-idf to them\n",
    "for cls in ['DummyStratified', 'MultinomialNB', 'ComplementNB']:\n",
    "    logger.info('-----------------------------------')\n",
    "    logger.info(f'Train for {cls}...')\n",
    "    Classifier = getattr(classifiers, cls)\n",
    "    model = Baseline(name=cls, classifier=Classifier)\n",
    "    model.fit(X_train, y_train)\n",
    "    models['raw'][cls] = model\n",
    "    all_scores['raw'][cls] = model.scores(X_test, y_test)\n",
    "    all_avg_scores['raw'][cls] = np.mean(all_scores['raw'][cls])\n",
    "    \n",
    "\n",
    "# decomposition methods\n",
    "decomps = {\n",
    "    'lsa_100': SVD(n_components=100),\n",
    "    'lsa_500': SVD(n_components=500),\n",
    "    'lsa_1k': SVD(n_components=1000),\n",
    "    'lda_100': [\n",
    "        SVD(n_components=1000),\n",
    "        Normalizer(),\n",
    "        LatentDirichletAllocation(n_components=100)\n",
    "    ],\n",
    "    'lda_500': [\n",
    "        SVD(n_components=1000),\n",
    "        Normalizer(),\n",
    "        LatentDirichletAllocation(n_components=500),\n",
    "    ]\n",
    "}\n",
    "\n",
    "for decomp, Decomposer in decomps.items():\n",
    "    logger.info('-----------------------------------')\n",
    "    logger.info(f'Build {decomp} features...')\n",
    "    pipe = FeaturePipeline(steps=Decomposer)\n",
    "    \n",
    "    # the decomposited features\n",
    "    X_train_ = pipe.fit_transform(X_train)\n",
    "    X_test_ = pipe.transform(X_test)\n",
    "    \n",
    "    # save how we decomped the features\n",
    "    models[decomp]['feature'] = pipe\n",
    "    \n",
    "    for cls in ['LDA', 'LinearSVC',\n",
    "                'Logistic', 'Ridge',\n",
    "                'ExtraTree']:\n",
    "        logger.info('-----------------------------------')\n",
    "        logger.info(f'Train for {decomp} -> {cls}...')\n",
    "        Classifier = getattr(classifiers, cls)\n",
    "        model = Baseline(name=cls, classifier=Classifier)\n",
    "        model.fit(X_train_, y_train)\n",
    "        models[decomp][cls] = model\n",
    "        all_scores[decomp][cls] = model.scores(X_test_, y_test)\n",
    "        all_avg_scores[decomp][cls] = np.mean(all_scores[decomp][cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
