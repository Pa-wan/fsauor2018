{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Engineering\n",
    "\n",
    "This notebook showcases different techniques for feature engineering and how they perform with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.getLogger('fgclassifier').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Format and Word Segmentation\n",
    "\n",
    "First, check how word segmentation works with the library we chose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-11 14:56:23,403 [INFO] Reading data/validate/sentiment_analysis_validationset.csv..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"哎，想当年来佘山的时候，啥都没有，三品香算镇上最大看起来最像样的饭店了。菜品多，有点太多，感觉啥都有，杂都不足以形容。随便点些，居然口味什么的都好还可以，价钱自然是便宜当震惊。元宝虾和椒盐九肚鱼都不错吃。不过近来几次么，味道明显没以前好了。冷餐里面一个凉拌海带丝还可以，酸酸甜甜的。镇上也有了些别的大点的饭店，所以不是每次必来了。对了，这家的生意一如既往的超级好，不定位基本吃不到。不过佘山这边的人吃晚饭很早的，所以稍微晚点去就很空了。\"\n",
      "\" 哎 ， 想当年 来 佘山 的 时候 ， 啥 都 没有 ， 三品 香算 镇上 最大 看起来 最 像样 的 饭店 了 。 菜品 多 ， 有点 太 多 ， 感觉 啥 都 有 ， 杂都 不足以 形容 。 随便 点些 ， 居然 口味 什么 的 都 好 还 可以 ， 价钱 自然 是 便宜 当 震惊 。 元宝 虾 和 椒盐 九肚鱼 都 不错 吃 。 不过 近来 几次 么 ， 味道 明显 没 以前 好 了 。 冷餐 里面 一个 凉拌 海带丝 还 可以 ， 酸酸甜甜 的 。 镇上 也 有 了 些 别的 大点 的 饭店 ， 所以 不是 每次 必来 了 。 对 了 ， 这家 的 生意 一如既往 的 超级 好 ， 不 定位 基本 吃 不到 。 不过 佘山 这边 的 人 吃晚饭 很早 的 ， 所以 稍微 晚点 去 就 很 空 了 。 \"\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from config import validate_data_path, train_data_path\n",
    "from fgclassifier import read_csv\n",
    "\n",
    "df = read_csv(validate_data_path, seg_words=False)\n",
    "\n",
    "print(df['content'][0])\n",
    "segs = jieba.lcut(df['content'][0])\n",
    "print(\" \".join(segs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '，', '来到', '北京', 'BBLANKK', 'BBLANKK', '清华大学']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace blank space with some materialized words\n",
    "jieba.add_word('BBLANKK')\n",
    "jieba.lcut(\"我，来到北京  清华大学\".replace(' ', 'BBLANKK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                          0\n",
       "content                                     \"哎，想当年来佘山的时候，啥都没有，三品香算镇上最大看起来最像样的饭店了。菜品多，有点太多，...\n",
       "location_traffic_convenience                                                               -2\n",
       "location_distance_from_business_district                                                   -2\n",
       "location_easy_to_find                                                                      -2\n",
       "service_wait_time                                                                           0\n",
       "service_waiters_attitude                                                                   -2\n",
       "service_parking_convenience                                                                -2\n",
       "service_serving_speed                                                                      -2\n",
       "price_level                                                                                 1\n",
       "price_cost_effective                                                                       -2\n",
       "price_discount                                                                             -2\n",
       "environment_decoration                                                                     -2\n",
       "environment_noise                                                                          -2\n",
       "environment_space                                                                          -2\n",
       "environment_cleaness                                                                       -2\n",
       "dish_portion                                                                               -2\n",
       "dish_taste                                                                                  0\n",
       "dish_look                                                                                  -2\n",
       "dish_recommendation                                                                        -2\n",
       "others_overall_experience                                                                   1\n",
       "others_willing_to_consume_again                                                             0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics\n",
    "\n",
    "First, check how many records we have. As word segmentation takes a while, we read the raw data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-11 14:56:23,766 [INFO] Reading data/train/sentiment_analysis_trainingset.csv..\n",
      "2018-11-11 14:56:25,451 [INFO] Reading data/validate/sentiment_analysis_validationset.csv..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (105000, 22)\n",
      "Validation data: (15000, 22)\n"
     ]
    }
   ],
   "source": [
    "from config import validate_data_path, train_data_path, testa_data_path\n",
    "\n",
    "# Without segmentation, this is faster\n",
    "df_train = read_csv(train_data_path, seg_words=False, sample_n=None)\n",
    "df_validate = read_csv(validate_data_path, seg_words=False, sample_n=None)\n",
    "print(\"Training data:\", df_train.shape)\n",
    "print(\"Validation data:\", df_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-11 14:56:25,754 [INFO] Reading data/test-a/sentiment_analysis_testa.csv..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-A data: (15000, 22)\n"
     ]
    }
   ],
   "source": [
    "df_testa = read_csv(testa_data_path, seg_words=False, sample_n=None)\n",
    "print(\"Test-A data:\", df_testa.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's check after segmentation, what does the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = read_csv(train_data_path, seg_words=True, sample_n=None)\n",
    "df_validate_full = read_csv(validate_data_path, seg_words=True, sample_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(content, counter=None):\n",
    "    counts = counter or Counter()\n",
    "    sentences = []\n",
    "    for s in content:\n",
    "        ss = s.split(' ')\n",
    "        sentences.append(ss)\n",
    "        counts.update(ss)\n",
    "    return counts, sentences\n",
    "\n",
    "count_train, sentences = count_words(df_train_full['content'])\n",
    "print(count_train.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "font = './misc/SourceHanSansHWSC/SourceHanSansHWSC-Regular.otf'\n",
    "wordcloud = WordCloud(\n",
    "    font_path=font, width=1200, height=800,\n",
    "    background_color='rgb(55, 71, 79)',\n",
    ").generate_from_frequencies(dict(count_train.most_common()[60:5000]))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([len(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(len(count_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({ 'values': list(count_train.values()) })\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['id', 'content'], axis=1).hist(figsize=(14, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.drop(['id', 'content'], axis=1).values.ravel()\n",
    "plt.hist(x)\n",
    "plt.xticks([-2, -1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from fgclassifier.baseline import Indie\n",
    "\n",
    "model = Indie()\n",
    "X_train, Y_train = model.load(config.train_data_path, sample_n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make English Translation\n",
    "\n",
    "To help non-English speakers understand the content, we make a subset of the training data with English translations.\n",
    "\n",
    "If you want to run the following code yourself, follow the instructions [here](https://cloud.google.com/translate/docs/quickstart-client-libraries#client-libraries-install-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-11-11 20:04:35,454 [DEBUG] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/6r/772b4sy16rg94jhq1fskv9fc0000gn/T/jieba.cache\n",
      "2018-11-11 20:04:35,457 [DEBUG] Loading model from cache /var/folders/6r/772b4sy16rg94jhq1fskv9fc0000gn/T/jieba.cache\n",
      "Loading model cost 0.828 seconds.\n",
      "2018-11-11 20:04:36,283 [DEBUG] Loading model cost 0.828 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-11-11 20:04:36,285 [DEBUG] Prefix dict has been built succesfully.\n",
      "2018-11-11 20:04:36,287 [INFO] Reading data/train/sentiment_analysis_trainingset.csv..\n"
     ]
    }
   ],
   "source": [
    "from config import validate_data_path, train_data_path, testa_data_path\n",
    "from fgclassifier import read_csv\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "df_train = read_csv(train_data_path, seg_words=False, sample_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    translations = joblib.load('data/train/en.pkl')\n",
    "except:\n",
    "    translations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:05<00:00,  1.24s/it] "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "from google.cloud import translate\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "# All available credentials\n",
    "credentials = glob.glob('./misc/google-cloud/*.json')\n",
    "\n",
    "# Use multiple credentials to bypass rate limit\n",
    "clients = []\n",
    "for credential in credentials:\n",
    "    print(credential)\n",
    "    clients.append(translate.Client.from_service_account_json(credential))\n",
    "\n",
    "df = df_train.copy().iloc[0:10000,:]\n",
    "contents = [x.strip('\"') for x in df['content']]\n",
    "n_client = len(clients)\n",
    "n_records = df.shape[0]\n",
    "\n",
    "client_ok = [True for _ in clients]\n",
    "\n",
    "\n",
    "def get_client(i):\n",
    "    c = 0\n",
    "    while not client_ok[i % n_client] and c < n_client:\n",
    "        c += 1\n",
    "        i += 1\n",
    "    i = i % n_client\n",
    "    client = clients[i] if c < n_client else None\n",
    "    return i, client\n",
    "\n",
    "failed = []\n",
    "\n",
    "clear_output()\n",
    "pbar = tqdm(total=n_records)\n",
    "queue = list(range(n_records))\n",
    "n_failed = 0\n",
    "\n",
    "while len(queue) and n_failed < n_client:\n",
    "    i = queue.pop(0)\n",
    "    if i not in translations:\n",
    "        start_time = time.time()\n",
    "        client_idx, client = get_client(i)\n",
    "        if not client:\n",
    "            raise RuntimeError('No Available Client.')\n",
    "        try:\n",
    "            translation = client.translate(contents[i],\n",
    "                target_language='en', source_language='zh')\n",
    "            translations[i] = translation['translatedText']\n",
    "        except Exception as e:\n",
    "            # print(client_idx + 1, e)\n",
    "            client_ok[client_idx] = False\n",
    "            queue.append(i)\n",
    "            n_failed += 1\n",
    "            continue\n",
    "        end_time = time.time()\n",
    "        # If finished within 1 second, wait...\n",
    "        if end_time < start_time + 0.5:\n",
    "            time.sleep(start_time + 0.5 - end_time)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_train.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(translations, 'data/train/en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace content with translation, and replace apostrophe \n",
    "df['content'] = [x.replace('&#39;', \"'\") for x in pd.Series(translations).sort_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"对于很多新店，确实要多个心眼。因为确实存在部分商家会制造虚假点评诱导消费者（譬如前不久的鱼当家），而且出于各种考虑，大众官方对于店家刷点评的事件经常是睁一只眼闭一只眼。所以我经常给小伙伴说，如果一家店（尤其是新店），有三条千万注意：1、仅有的几条到十几条评价都是全五星。2、这些点评在几天内密集出现。3、点评号的级别大都低于三颗星，且有大量系统自动分配账号，无VIP账号点评。那么这家店很可能是在刷点评。而这样的店家通常都有一个共同的特点，就是比较容易着急（用词很委婉了哦：）。须知，重视顾客点评本身是好事，但是饮食业要想做好还是那三条：口味好、服务好、性价比高。这三条做好了，那么酒香不怕巷子深，生意自然会好，体验较好的会员大都会本着良心点评。而如果脱离这个，只想靠刷点评，就好比雇人发传单，知道的人固然多了，但是最重要的三条跟不上，那么只能是一锤子买卖——可能有人会说，南京这么大，我一锤子买卖也能赚好多了呢——须知，一个一锤子买卖积累起来，负面评价会很快冲掉刷的评价，最终的结果不言而喻。——当然，按照店家的逻辑，我在体验前似乎也不应该写点评。不过请放心，大叔已经购买了团购券，会在一周内探店的，到时候自然会根据体验情况如实地更改评价。\"\n",
      "For many new stores, it really takes a lot of attention. Because there are some merchants who will create false reviews to induce consumers (such as the fisherman's head of the past), and for various reasons, the public official's comments on the store's comments are often one eye closed. Therefore, I often tell my friends that if a store (especially a new store) has three items to pay attention to: 1. Only a few to a dozen ratings are all five stars. 2. These reviews appear intensively within a few days. 3, the level of the rating is mostly lower than three stars, and a large number of systems automatically assign accounts, no VIP account comments. Then this store is probably in the brush review. And such a store usually has a common feature, that is, it is easier to worry (the word is euphemistic:). It should be noted that it is good to pay attention to customer reviews, but the catering industry should do three things: good taste, good service and high cost performance. These three items are done well, so the wine is not afraid of the alleys, the business will naturally be good, and the members who experience better will be judged by their conscience. And if you leave this, you just want to rely on the brush to comment, it is like hiring people to send flyers, knowing a lot of people, but the most important three can not keep up, then only a hammer sale - maybe someone will say, Nanjing Big, I can make a lot of money by buying and selling a hammer - I need to know that a one-hammer sale will accumulate, and the negative evaluation will quickly wash away the evaluation of the brush. The final result is self-evident. - Of course, according to the logic of the store, I don't seem to be able to write reviews before the experience. However, please be assured that Uncle has already purchased a group purchase voucher and will visit the store within one week. At that time, it will naturally change the evaluation according to the experience.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print(df_train['content'][9999])\n",
    "print(df['content'][9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>Hey, the lollipop of the dead man, the overlor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_wait_time</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_serving_speed</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_level</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_cost_effective</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_discount</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_decoration</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_noise</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_space</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_cleaness</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_portion</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_taste</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_look</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_recommendation</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others_overall_experience</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          0\n",
       "id                                                                                        0\n",
       "content                                   Hey, the lollipop of the dead man, the overlor...\n",
       "location_traffic_convenience                                                             -2\n",
       "location_distance_from_business_district                                                 -2\n",
       "location_easy_to_find                                                                    -2\n",
       "service_wait_time                                                                        -2\n",
       "service_waiters_attitude                                                                  1\n",
       "service_parking_convenience                                                              -2\n",
       "service_serving_speed                                                                    -2\n",
       "price_level                                                                              -2\n",
       "price_cost_effective                                                                     -2\n",
       "price_discount                                                                            1\n",
       "environment_decoration                                                                   -2\n",
       "environment_noise                                                                        -2\n",
       "environment_space                                                                        -2\n",
       "environment_cleaness                                                                     -2\n",
       "dish_portion                                                                             -2\n",
       "dish_taste                                                                               -2\n",
       "dish_look                                                                                 1\n",
       "dish_recommendation                                                                      -2\n",
       "others_overall_experience                                                                 1\n",
       "others_willing_to_consume_again                                                          -2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import csv\n",
    "\n",
    "# Sample data obtained by Google Translating to English\n",
    "df.to_csv('data/english.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
