{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Engineering\n",
    "\n",
    "This notebook showcases different techniques for feature engineering and how they perform with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.getLogger('fgclassifier').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Format and Word Segmentation\n",
    "\n",
    "First, check how word segmentation works with the library we chose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-11 14:56:23,403 [INFO] Reading data/validate/sentiment_analysis_validationset.csv..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"哎，想当年来佘山的时候，啥都没有，三品香算镇上最大看起来最像样的饭店了。菜品多，有点太多，感觉啥都有，杂都不足以形容。随便点些，居然口味什么的都好还可以，价钱自然是便宜当震惊。元宝虾和椒盐九肚鱼都不错吃。不过近来几次么，味道明显没以前好了。冷餐里面一个凉拌海带丝还可以，酸酸甜甜的。镇上也有了些别的大点的饭店，所以不是每次必来了。对了，这家的生意一如既往的超级好，不定位基本吃不到。不过佘山这边的人吃晚饭很早的，所以稍微晚点去就很空了。\"\n",
      "\" 哎 ， 想当年 来 佘山 的 时候 ， 啥 都 没有 ， 三品 香算 镇上 最大 看起来 最 像样 的 饭店 了 。 菜品 多 ， 有点 太 多 ， 感觉 啥 都 有 ， 杂都 不足以 形容 。 随便 点些 ， 居然 口味 什么 的 都 好 还 可以 ， 价钱 自然 是 便宜 当 震惊 。 元宝 虾 和 椒盐 九肚鱼 都 不错 吃 。 不过 近来 几次 么 ， 味道 明显 没 以前 好 了 。 冷餐 里面 一个 凉拌 海带丝 还 可以 ， 酸酸甜甜 的 。 镇上 也 有 了 些 别的 大点 的 饭店 ， 所以 不是 每次 必来 了 。 对 了 ， 这家 的 生意 一如既往 的 超级 好 ， 不 定位 基本 吃 不到 。 不过 佘山 这边 的 人 吃晚饭 很早 的 ， 所以 稍微 晚点 去 就 很 空 了 。 \"\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from config import validate_data_path, train_data_path\n",
    "from fgclassifier import read_csv\n",
    "\n",
    "df = read_csv(validate_data_path, seg_words=False)\n",
    "\n",
    "print(df['content'][0])\n",
    "segs = jieba.lcut(df['content'][0])\n",
    "print(\" \".join(segs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '，', '来到', '北京', 'BBLANKK', 'BBLANKK', '清华大学']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace blank space with some materialized words\n",
    "jieba.add_word('BBLANKK')\n",
    "jieba.lcut(\"我，来到北京  清华大学\".replace(' ', 'BBLANKK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                          0\n",
       "content                                     \"哎，想当年来佘山的时候，啥都没有，三品香算镇上最大看起来最像样的饭店了。菜品多，有点太多，...\n",
       "location_traffic_convenience                                                               -2\n",
       "location_distance_from_business_district                                                   -2\n",
       "location_easy_to_find                                                                      -2\n",
       "service_wait_time                                                                           0\n",
       "service_waiters_attitude                                                                   -2\n",
       "service_parking_convenience                                                                -2\n",
       "service_serving_speed                                                                      -2\n",
       "price_level                                                                                 1\n",
       "price_cost_effective                                                                       -2\n",
       "price_discount                                                                             -2\n",
       "environment_decoration                                                                     -2\n",
       "environment_noise                                                                          -2\n",
       "environment_space                                                                          -2\n",
       "environment_cleaness                                                                       -2\n",
       "dish_portion                                                                               -2\n",
       "dish_taste                                                                                  0\n",
       "dish_look                                                                                  -2\n",
       "dish_recommendation                                                                        -2\n",
       "others_overall_experience                                                                   1\n",
       "others_willing_to_consume_again                                                             0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics\n",
    "\n",
    "First, check how many records we have. As word segmentation takes a while, we read the raw data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-11 14:56:23,766 [INFO] Reading data/train/sentiment_analysis_trainingset.csv..\n",
      "2018-11-11 14:56:25,451 [INFO] Reading data/validate/sentiment_analysis_validationset.csv..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (105000, 22)\n",
      "Validation data: (15000, 22)\n"
     ]
    }
   ],
   "source": [
    "from config import validate_data_path, train_data_path, testa_data_path\n",
    "\n",
    "# Without segmentation, this is faster\n",
    "df_train = read_csv(train_data_path, seg_words=False, sample_n=None)\n",
    "df_validate = read_csv(validate_data_path, seg_words=False, sample_n=None)\n",
    "print(\"Training data:\", df_train.shape)\n",
    "print(\"Validation data:\", df_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-11 14:56:25,754 [INFO] Reading data/test-a/sentiment_analysis_testa.csv..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-A data: (15000, 22)\n"
     ]
    }
   ],
   "source": [
    "df_testa = read_csv(testa_data_path, seg_words=False, sample_n=None)\n",
    "print(\"Test-A data:\", df_testa.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's check after segmentation, what does the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = read_csv(train_data_path, seg_words=True, sample_n=None)\n",
    "df_validate_full = read_csv(validate_data_path, seg_words=True, sample_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(content, counter=None):\n",
    "    counts = counter or Counter()\n",
    "    sentences = []\n",
    "    for s in content:\n",
    "        ss = s.split(' ')\n",
    "        sentences.append(ss)\n",
    "        counts.update(ss)\n",
    "    return counts, sentences\n",
    "\n",
    "count_train, sentences = count_words(df_train_full['content'])\n",
    "print(count_train.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "font = './misc/SourceHanSansHWSC/SourceHanSansHWSC-Regular.otf'\n",
    "wordcloud = WordCloud(\n",
    "    font_path=font, width=1200, height=800,\n",
    "    background_color='rgb(55, 71, 79)',\n",
    ").generate_from_frequencies(dict(count_train.most_common()[60:5000]))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([len(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(len(count_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({ 'values': list(count_train.values()) })\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['id', 'content'], axis=1).hist(figsize=(14, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.drop(['id', 'content'], axis=1).values.ravel()\n",
    "plt.hist(x)\n",
    "plt.xticks([-2, -1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from fgclassifier.baseline import Indie\n",
    "\n",
    "model = Indie()\n",
    "X_train, Y_train = model.load(config.train_data_path, sample_n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make English Translation\n",
    "\n",
    "To help non-English speakers understand the content, we make a subset of the training data with English translations.\n",
    "\n",
    "If you want to run the following code yourself, follow the instructions [here](https://cloud.google.com/translate/docs/quickstart-client-libraries#client-libraries-install-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-11-11 20:04:35,454 [DEBUG] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/6r/772b4sy16rg94jhq1fskv9fc0000gn/T/jieba.cache\n",
      "2018-11-11 20:04:35,457 [DEBUG] Loading model from cache /var/folders/6r/772b4sy16rg94jhq1fskv9fc0000gn/T/jieba.cache\n",
      "Loading model cost 0.828 seconds.\n",
      "2018-11-11 20:04:36,283 [DEBUG] Loading model cost 0.828 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-11-11 20:04:36,285 [DEBUG] Prefix dict has been built succesfully.\n",
      "2018-11-11 20:04:36,287 [INFO] Reading data/train/sentiment_analysis_trainingset.csv..\n"
     ]
    }
   ],
   "source": [
    "from config import validate_data_path, train_data_path, testa_data_path\n",
    "from fgclassifier import read_csv\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "df_train = read_csv(train_data_path, seg_words=False, sample_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    translations = joblib.load('data/train/en.pkl')\n",
    "except:\n",
    "    translations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:05<00:00,  1.24s/it] "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "from google.cloud import translate\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "# All available credentials\n",
    "credentials = glob.glob('./misc/google-cloud/*.json')\n",
    "\n",
    "# Use multiple credentials to bypass rate limit\n",
    "clients = []\n",
    "for credential in credentials:\n",
    "    print(credential)\n",
    "    clients.append(translate.Client.from_service_account_json(credential))\n",
    "\n",
    "df = df_train.copy().iloc[0:10000,:]\n",
    "contents = [x.strip('\"') for x in df['content']]\n",
    "n_client = len(clients)\n",
    "n_records = df.shape[0]\n",
    "\n",
    "client_ok = [True for _ in clients]\n",
    "\n",
    "\n",
    "def get_client(i):\n",
    "    c = 0\n",
    "    while not client_ok[i % n_client] and c < n_client:\n",
    "        c += 1\n",
    "        i += 1\n",
    "    i = i % n_client\n",
    "    client = clients[i] if c < n_client else None\n",
    "    return i, client\n",
    "\n",
    "failed = []\n",
    "\n",
    "clear_output()\n",
    "pbar = tqdm(total=n_records)\n",
    "queue = list(range(n_records))\n",
    "n_failed = 0\n",
    "\n",
    "while len(queue) and n_failed < n_client:\n",
    "    i = queue.pop(0)\n",
    "    if i not in translations:\n",
    "        start_time = time.time()\n",
    "        client_idx, client = get_client(i)\n",
    "        if not client:\n",
    "            raise RuntimeError('No Available Client.')\n",
    "        try:\n",
    "            translation = client.translate(contents[i],\n",
    "                target_language='en', source_language='zh')\n",
    "            translations[i] = translation['translatedText']\n",
    "        except Exception as e:\n",
    "            # print(client_idx + 1, e)\n",
    "            client_ok[client_idx] = False\n",
    "            queue.append(i)\n",
    "            n_failed += 1\n",
    "            continue\n",
    "        end_time = time.time()\n",
    "        # If finished within 1 second, wait...\n",
    "        if end_time < start_time + 0.5:\n",
    "            time.sleep(start_time + 0.5 - end_time)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_train.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(translations, 'data/train/en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace content with translation, and replace apostrophe \n",
    "df['content'] = [x.replace('&#39;', \"'\") for x in pd.Series(translations).sort_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"【试吃点评】\n",
      "【环境评价】坐落于一方莲花之中，转角之间悄然而见，似是脱俗，似是悠然。居于莲花大厦小区内，如若细心，不难发现。庭院之内，摆放着几张小桌，些许椅子，墨绿的遮阳篷下，几盏小茶，一个午后！\n",
      "缓步入内，几个书架隔出了几个雅座，一席帘布，营造了小小的空间，或缱绻或仰卧，几多自由，一本闲书，一个时光！\n",
      "【餐品评价】\n",
      "【菊花茶】一盏花茶，淡淡的，虽没有多大特色，也就几许菊花，一壶开水，可是午后也该是慵懒的，回味间，菊花香阵阵而来，齿间也是舒适的。\n",
      "【玫瑰花】于心，玫瑰于女生还是美颜之物，虽不知其中道理，可也是众星捧月的花茶之一。午后来一盏也是闲适之物，也给自己放个假。\n",
      "【】落入盘中，一朵薄荷叶立中间，简单而富有画面，落入口中也是口感细腻，不会甜腻，一层【】有点微微苦感，却将本身微调出一种细细的美感，而我也钟爱这款甜点！\n",
      "【榴莲班戟】一层薄薄的蛋皮，包裹住一腔美味，淡淡的奶油，不腻，口感细滑，据店家介绍，也是顶好的奶油调制而成，想来也是颇费苦心制作每一款产品。切成些许小块，更是很符合需求，不过大厨也说这个切的会些许麻烦，也可见其刀工。\n",
      "【鸡蛋布丁】细滑的口感，精致的装盘，小小容器内足见一方明月，奶香十足，满口淡淡的香气，落入口中也是一佳品，据传这也是店内热销产品，也可见其口感之佳！\n",
      "【综合评价】店内还有提供饭点，观其价格还是相对可以的，午后来次，一本闲书，一个午后，一盏花茶，一款甜点，足以！\"\n",
      "[Try to eat reviews] [Environmental evaluation] is located in a lotus flower, and the corners are quietly seen, it seems to be refined, it seems to be leisurely. Living in the Lotus Building, if you are careful, it is not difficult to find. Inside the courtyard, there are a few small tables, a few chairs, a dark green awning, a few small teas, one afternoon! Stepping into the room, several bookshelves are separated by a few seats, a curtain, creating a small space, or lying or lying on the back, how many free, a book, a time! [Evaluation of meals] [Chrysanthemum tea] A scent of tea, a touch of light, although there are not many characteristics, there are a few chrysanthemums, a pot of boiling water, but it should be lazy in the afternoon, after the aftertaste, chrysanthemum fragrance bursts, teeth The room is also comfortable. [Rose] In the heart, the rose is still a girl in the beauty, although I don’t know the truth, it is also one of the flower teas that the stars hold. In the afternoon, I was also a leisurely thing, and I also gave myself a vacation. [] falls into the plate, a mint leaf stands in the middle, simple and rich in the picture, the mouth is also delicate and not sweet, the layer [] is slightly bitter, but it will fine-tune itself a fine Beauty, and I love this dessert! [Durian Banyan] A thin layer of egg skin, wrapped in a delicious cavity, a touch of cream, not greasy, smooth taste, according to the store introduction, is also a good cream modulation, it is also painstaking to make each A product. Cut into small pieces, it is very suitable for the needs, but the chef also said that this cut will be a little trouble, but also its knife. [Egg pudding] The smooth taste, exquisite plate, the small container inside the foot see a moon, full of milk, full of a touch of aroma, is also a good product in the entrance, it is said that this is also the best selling products in the store, it can also be seen Good taste! [Comprehensive evaluation] There is also a meal in the store, and the price is relatively good. Afternoon, a book, an afternoon, a cup of tea, a dessert, enough!\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "import numpy as np\n",
    "\n",
    "idx = np.random.randint(0, 10000)\n",
    "print(df_train['content'][idx])\n",
    "print(df['content'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>Hey, the lollipop of the dead man, the overlor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_wait_time</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service_serving_speed</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_level</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_cost_effective</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_discount</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_decoration</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_noise</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_space</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment_cleaness</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_portion</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_taste</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_look</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dish_recommendation</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others_overall_experience</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          0\n",
       "id                                                                                        0\n",
       "content                                   Hey, the lollipop of the dead man, the overlor...\n",
       "location_traffic_convenience                                                             -2\n",
       "location_distance_from_business_district                                                 -2\n",
       "location_easy_to_find                                                                    -2\n",
       "service_wait_time                                                                        -2\n",
       "service_waiters_attitude                                                                  1\n",
       "service_parking_convenience                                                              -2\n",
       "service_serving_speed                                                                    -2\n",
       "price_level                                                                              -2\n",
       "price_cost_effective                                                                     -2\n",
       "price_discount                                                                            1\n",
       "environment_decoration                                                                   -2\n",
       "environment_noise                                                                        -2\n",
       "environment_space                                                                        -2\n",
       "environment_cleaness                                                                     -2\n",
       "dish_portion                                                                             -2\n",
       "dish_taste                                                                               -2\n",
       "dish_look                                                                                 1\n",
       "dish_recommendation                                                                      -2\n",
       "others_overall_experience                                                                 1\n",
       "others_willing_to_consume_again                                                          -2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import csv\n",
    "\n",
    "# Sample data obtained by Google Translating to English\n",
    "df.to_csv('data/english.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
